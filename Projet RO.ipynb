{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentification YouTube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = \"AIzaSyDtM6sAMISpPjHUxwmhRpx-u7yDCh5lQ8Q\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "    developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les Vidéos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récuperation des vidéos\n",
    "list_videos = youtube.search().list(q=\"Qatar Airways\",type=\"video\",order = \"relevance\",maxResults=5,\n",
    "                               part=\"id,snippet\").execute().get('items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kind': 'youtube#searchResult',\n",
       "  'etag': '\"p4VTdlkQv3HQeTEaXgvLePAydmU/D7cBZ0pySXAn4FP59h-r1hd9os8\"',\n",
       "  'id': {'kind': 'youtube#video', 'videoId': 'L9WQe6T7kiw'},\n",
       "  'snippet': {'publishedAt': '2019-06-15T11:00:02.000Z',\n",
       "   'channelId': 'UC0NdkLdik4nyugI9AqlOtYg',\n",
       "   'title': '16 HOURS in ECONOMY CLASS | Qatar Airways | Boeing 777-200LR | Doha - Auckland',\n",
       "   'description': 'More Trip Reports/ Flight Reviews: https://www.youtube.com/watch?v=S1TLrttEboI&list=PLVg0qHewng_viczmYzVBjz3Te1sID6tT3 ••••••••••••••••••••••••...',\n",
       "   'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/L9WQe6T7kiw/default.jpg',\n",
       "     'width': 120,\n",
       "     'height': 90},\n",
       "    'medium': {'url': 'https://i.ytimg.com/vi/L9WQe6T7kiw/mqdefault.jpg',\n",
       "     'width': 320,\n",
       "     'height': 180},\n",
       "    'high': {'url': 'https://i.ytimg.com/vi/L9WQe6T7kiw/hqdefault.jpg',\n",
       "     'width': 480,\n",
       "     'height': 360}},\n",
       "   'channelTitle': 'FlightExperience',\n",
       "   'liveBroadcastContent': 'none'}},\n",
       " {'kind': 'youtube#searchResult',\n",
       "  'etag': '\"p4VTdlkQv3HQeTEaXgvLePAydmU/qjttRf0d9eZkUGXUlt7Artz6AD8\"',\n",
       "  'id': {'kind': 'youtube#video', 'videoId': 'W3B7kPh-mcg'},\n",
       "  'snippet': {'publishedAt': '2018-02-23T13:00:11.000Z',\n",
       "   'channelId': 'UCfYCRj25JJQ41JGPqiqXmJw',\n",
       "   'title': 'Qatar Airways The World&#39;s FIRST A350-1000 Flight',\n",
       "   'description': \"Qatar Airways is the World's first launch customer for A350-1000 series. This video document the first flight of Qatar Airways A350-1000. Check out the ...\",\n",
       "   'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/W3B7kPh-mcg/default.jpg',\n",
       "     'width': 120,\n",
       "     'height': 90},\n",
       "    'medium': {'url': 'https://i.ytimg.com/vi/W3B7kPh-mcg/mqdefault.jpg',\n",
       "     'width': 320,\n",
       "     'height': 180},\n",
       "    'high': {'url': 'https://i.ytimg.com/vi/W3B7kPh-mcg/hqdefault.jpg',\n",
       "     'width': 480,\n",
       "     'height': 360}},\n",
       "   'channelTitle': 'Sam Chui',\n",
       "   'liveBroadcastContent': 'none'}},\n",
       " {'kind': 'youtube#searchResult',\n",
       "  'etag': '\"p4VTdlkQv3HQeTEaXgvLePAydmU/piCxm2gJWL94MXxarOvPo9--t6g\"',\n",
       "  'id': {'kind': 'youtube#video', 'videoId': 'OT3CLInWrk4'},\n",
       "  'snippet': {'publishedAt': '2019-11-28T12:12:11.000Z',\n",
       "   'channelId': 'UCi8xUU_lg3zr8UcBXLdEheQ',\n",
       "   'title': 'A Safety Video Like Never Before | Qatar Airways',\n",
       "   'description': \"QatarAirways is excited to launch its new, star-studded on-board Safety Video. Featuring FC Bayern Munchen's prolific goalscorer Robert Lewandowski, Brazil ...\",\n",
       "   'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/OT3CLInWrk4/default.jpg',\n",
       "     'width': 120,\n",
       "     'height': 90},\n",
       "    'medium': {'url': 'https://i.ytimg.com/vi/OT3CLInWrk4/mqdefault.jpg',\n",
       "     'width': 320,\n",
       "     'height': 180},\n",
       "    'high': {'url': 'https://i.ytimg.com/vi/OT3CLInWrk4/hqdefault.jpg',\n",
       "     'width': 480,\n",
       "     'height': 360}},\n",
       "   'channelTitle': 'Qatar Airways',\n",
       "   'liveBroadcastContent': 'none'}},\n",
       " {'kind': 'youtube#searchResult',\n",
       "  'etag': '\"p4VTdlkQv3HQeTEaXgvLePAydmU/4zJ7TGqPQ_CJKod1RUKVrEdiPlE\"',\n",
       "  'id': {'kind': 'youtube#video', 'videoId': 'tHwIf9KQhAA'},\n",
       "  'snippet': {'publishedAt': '2018-09-22T03:27:04.000Z',\n",
       "   'channelId': 'UCZ9k_vOGO1WomX6WKyG2teg',\n",
       "   'title': 'QATAR AIRWAYS CABIN CREW LIFE - Documentary',\n",
       "   'description': 'QATAR AIRWAYS CABIN CREW LIFE - Documentary This video is made for educational purposes only.',\n",
       "   'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/tHwIf9KQhAA/default.jpg',\n",
       "     'width': 120,\n",
       "     'height': 90},\n",
       "    'medium': {'url': 'https://i.ytimg.com/vi/tHwIf9KQhAA/mqdefault.jpg',\n",
       "     'width': 320,\n",
       "     'height': 180},\n",
       "    'high': {'url': 'https://i.ytimg.com/vi/tHwIf9KQhAA/hqdefault.jpg',\n",
       "     'width': 480,\n",
       "     'height': 360}},\n",
       "   'channelTitle': 'I want to be a cabin crew',\n",
       "   'liveBroadcastContent': 'none'}},\n",
       " {'kind': 'youtube#searchResult',\n",
       "  'etag': '\"p4VTdlkQv3HQeTEaXgvLePAydmU/j7mbjfyZ-e0eDEoRtX7zFfROG08\"',\n",
       "  'id': {'kind': 'youtube#video', 'videoId': 'EW9ffBse6Nc'},\n",
       "  'snippet': {'publishedAt': '2018-11-04T02:40:01.000Z',\n",
       "   'channelId': 'UCZ9k_vOGO1WomX6WKyG2teg',\n",
       "   'title': 'Qatar Airways Cabin Crew - HOW TO GET THE JOB(TIPS)',\n",
       "   'description': 'Qatar Airways Cabin Crew - TIPS This video has educational purposes only. Requirements: https://youtu.be/tXhpOjn1gUo CV sample and tips: ...',\n",
       "   'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/EW9ffBse6Nc/default.jpg',\n",
       "     'width': 120,\n",
       "     'height': 90},\n",
       "    'medium': {'url': 'https://i.ytimg.com/vi/EW9ffBse6Nc/mqdefault.jpg',\n",
       "     'width': 320,\n",
       "     'height': 180},\n",
       "    'high': {'url': 'https://i.ytimg.com/vi/EW9ffBse6Nc/hqdefault.jpg',\n",
       "     'width': 480,\n",
       "     'height': 360}},\n",
       "   'channelTitle': 'I want to be a cabin crew',\n",
       "   'liveBroadcastContent': 'none'}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    'title':[],\n",
    "    'channelTitle':[],\n",
    "    'description':[],\n",
    "    'videoId':[],\n",
    "    'publishedAt':[]\n",
    "}\n",
    "\n",
    "l = len(list_videos)\n",
    "\n",
    "for i,item in enumerate(list_videos):\n",
    "    data['title'].append(item['snippet']['title'])\n",
    "    data['channelTitle'].append(item['snippet']['channelTitle'])\n",
    "    data['description'].append(item['snippet']['description'])\n",
    "    data['videoId'].append(item['id']['videoId'])\n",
    "    data['publishedAt'].append(item['snippet']['publishedAt'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>description</th>\n",
       "      <th>videoId</th>\n",
       "      <th>publishedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16 HOURS in ECONOMY CLASS | Qatar Airways | Bo...</td>\n",
       "      <td>FlightExperience</td>\n",
       "      <td>More Trip Reports/ Flight Reviews: https://www...</td>\n",
       "      <td>L9WQe6T7kiw</td>\n",
       "      <td>2019-06-15T11:00:02.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qatar Airways The World&amp;#39;s FIRST A350-1000 ...</td>\n",
       "      <td>Sam Chui</td>\n",
       "      <td>Qatar Airways is the World's first launch cust...</td>\n",
       "      <td>W3B7kPh-mcg</td>\n",
       "      <td>2018-02-23T13:00:11.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Safety Video Like Never Before | Qatar Airways</td>\n",
       "      <td>Qatar Airways</td>\n",
       "      <td>QatarAirways is excited to launch its new, sta...</td>\n",
       "      <td>OT3CLInWrk4</td>\n",
       "      <td>2019-11-28T12:12:11.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QATAR AIRWAYS CABIN CREW LIFE - Documentary</td>\n",
       "      <td>I want to be a cabin crew</td>\n",
       "      <td>QATAR AIRWAYS CABIN CREW LIFE - Documentary Th...</td>\n",
       "      <td>tHwIf9KQhAA</td>\n",
       "      <td>2018-09-22T03:27:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qatar Airways Cabin Crew - HOW TO GET THE JOB(...</td>\n",
       "      <td>I want to be a cabin crew</td>\n",
       "      <td>Qatar Airways Cabin Crew - TIPS This video has...</td>\n",
       "      <td>EW9ffBse6Nc</td>\n",
       "      <td>2018-11-04T02:40:01.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  16 HOURS in ECONOMY CLASS | Qatar Airways | Bo...   \n",
       "1  Qatar Airways The World&#39;s FIRST A350-1000 ...   \n",
       "2   A Safety Video Like Never Before | Qatar Airways   \n",
       "3        QATAR AIRWAYS CABIN CREW LIFE - Documentary   \n",
       "4  Qatar Airways Cabin Crew - HOW TO GET THE JOB(...   \n",
       "\n",
       "                channelTitle  \\\n",
       "0           FlightExperience   \n",
       "1                   Sam Chui   \n",
       "2              Qatar Airways   \n",
       "3  I want to be a cabin crew   \n",
       "4  I want to be a cabin crew   \n",
       "\n",
       "                                         description      videoId  \\\n",
       "0  More Trip Reports/ Flight Reviews: https://www...  L9WQe6T7kiw   \n",
       "1  Qatar Airways is the World's first launch cust...  W3B7kPh-mcg   \n",
       "2  QatarAirways is excited to launch its new, sta...  OT3CLInWrk4   \n",
       "3  QATAR AIRWAYS CABIN CREW LIFE - Documentary Th...  tHwIf9KQhAA   \n",
       "4  Qatar Airways Cabin Crew - TIPS This video has...  EW9ffBse6Nc   \n",
       "\n",
       "                publishedAt  \n",
       "0  2019-06-15T11:00:02.000Z  \n",
       "1  2018-02-23T13:00:11.000Z  \n",
       "2  2019-11-28T12:12:11.000Z  \n",
       "3  2018-09-22T03:27:04.000Z  \n",
       "4  2018-11-04T02:40:01.000Z  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.to_csv('QA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaires On QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_data={\n",
    "    'id':[],\n",
    "    'likeCount':[],\n",
    "    'textDisplay':[],\n",
    "    'videoId':[],\n",
    "    'authorDisplayName':[],\n",
    "    'authorChannelId':[],\n",
    "    'publishedAt':[]\n",
    "}\n",
    "l = len(list_videos)\n",
    "\n",
    "\n",
    "\n",
    "for i,item in enumerate(list_videos):\n",
    "    \n",
    "        threads = youtube.commentThreads().list( \n",
    "            videoId=item['id']['videoId'] ,  \n",
    "            part='snippet,replies',\n",
    "            maxResults=100\n",
    "        ).execute().get('items')\n",
    "        for thread in threads:\n",
    "            comments_data['id'].append(thread['id'])\n",
    "            comments_data['likeCount'].append(thread['snippet']['topLevelComment']['snippet']['likeCount'])\n",
    "            comments_data['textDisplay'].append(thread['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "            comments_data['videoId'].append(thread['snippet']['videoId'])\n",
    "            comments_data['publishedAt'].append(thread['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "            comments_data['authorChannelId'].append(thread['snippet']['topLevelComment']['snippet']['authorChannelId']['value'])\n",
    "            comments_data['authorDisplayName'].append(thread['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
    "        \n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.DataFrame(data=comments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>videoId</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>publishedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugwoz9MsSiNr9sY_evh4AaABAg</td>\n",
       "      <td>0</td>\n",
       "      <td>ACTUALLY HOW MANY MEALS DID U GET</td>\n",
       "      <td>L9WQe6T7kiw</td>\n",
       "      <td>strike sisters</td>\n",
       "      <td>UCvITJcoiJc8Jf4hQ38nLOiw</td>\n",
       "      <td>2019-12-11T18:00:30.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgwMiNx4yafLfcFgsQZ4AaABAg</td>\n",
       "      <td>0</td>\n",
       "      <td>12 hr  Cairo to NYC</td>\n",
       "      <td>L9WQe6T7kiw</td>\n",
       "      <td>Omar Elgamal</td>\n",
       "      <td>UCPiRtnyNkv0ikzp8w0P6hOw</td>\n",
       "      <td>2019-12-08T18:39:25.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgxLslKHRXROWnIat-14AaABAg</td>\n",
       "      <td>2</td>\n",
       "      <td>At &lt;a href=\"https://www.youtube.com/watch?v=L9...</td>\n",
       "      <td>L9WQe6T7kiw</td>\n",
       "      <td>Emme Geffrard</td>\n",
       "      <td>UC6wwROoVpbmMv7lBHa55VwA</td>\n",
       "      <td>2019-12-07T20:40:24.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgxOg7EhOEFibddaXFN4AaABAg</td>\n",
       "      <td>0</td>\n",
       "      <td>How do you know about the plane’s informtion?</td>\n",
       "      <td>L9WQe6T7kiw</td>\n",
       "      <td>Opening and Closing Logos</td>\n",
       "      <td>UCc42cOwPFV68ivru46YSyZQ</td>\n",
       "      <td>2019-12-07T12:57:28.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UgxJSV0ox-PvHee0ceZ4AaABAg</td>\n",
       "      <td>0</td>\n",
       "      <td>My longes flight was from Manchester to Islama...</td>\n",
       "      <td>L9WQe6T7kiw</td>\n",
       "      <td>Owais Hussain</td>\n",
       "      <td>UCwmIrn-08kM7esLRSwPJT-w</td>\n",
       "      <td>2019-12-07T11:44:58.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  likeCount  \\\n",
       "0  Ugwoz9MsSiNr9sY_evh4AaABAg          0   \n",
       "1  UgwMiNx4yafLfcFgsQZ4AaABAg          0   \n",
       "2  UgxLslKHRXROWnIat-14AaABAg          2   \n",
       "3  UgxOg7EhOEFibddaXFN4AaABAg          0   \n",
       "4  UgxJSV0ox-PvHee0ceZ4AaABAg          0   \n",
       "\n",
       "                                         textDisplay      videoId  \\\n",
       "0                  ACTUALLY HOW MANY MEALS DID U GET  L9WQe6T7kiw   \n",
       "1                                12 hr  Cairo to NYC  L9WQe6T7kiw   \n",
       "2  At <a href=\"https://www.youtube.com/watch?v=L9...  L9WQe6T7kiw   \n",
       "3      How do you know about the plane’s informtion?  L9WQe6T7kiw   \n",
       "4  My longes flight was from Manchester to Islama...  L9WQe6T7kiw   \n",
       "\n",
       "           authorDisplayName           authorChannelId  \\\n",
       "0             strike sisters  UCvITJcoiJc8Jf4hQ38nLOiw   \n",
       "1               Omar Elgamal  UCPiRtnyNkv0ikzp8w0P6hOw   \n",
       "2              Emme Geffrard  UC6wwROoVpbmMv7lBHa55VwA   \n",
       "3  Opening and Closing Logos  UCc42cOwPFV68ivru46YSyZQ   \n",
       "4              Owais Hussain  UCwmIrn-08kM7esLRSwPJT-w   \n",
       "\n",
       "                publishedAt  \n",
       "0  2019-12-11T18:00:30.000Z  \n",
       "1  2019-12-08T18:39:25.000Z  \n",
       "2  2019-12-07T20:40:24.000Z  \n",
       "3  2019-12-07T12:57:28.000Z  \n",
       "4  2019-12-07T11:44:58.000Z  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.to_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle d'Analyse Sentimentale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Mensi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-11a76bd8c146>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMensi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mQA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Keeping only the neccessary columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Mensi' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Mensi/QA.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-3591102a07a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No neutral comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-1ac010b00dc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Splitting the dataset into train and test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Removing neutral sentiments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"Neutral\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2124\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2124\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into train and test set\n",
    "train, test = train_test_split(data,test_size = 0.1)\n",
    "# Removing neutral sentiments\n",
    "train = train[train.sentiment != \"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récuperation des text et séparation par polarité\n",
    "train_pos = train[ train['sentiment'] == 'Positive']\n",
    "train_pos = train_pos['text']\n",
    "train_neg = train[ train['sentiment'] == 'Negative']\n",
    "train_neg = train_neg['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour afficher un nuage de mots\n",
    "def wordcloud_draw(data, color = 'black'):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('#')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive words\")\n",
    "wordcloud_draw(train_pos,'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to notice the following words and expressions in the positive word set: truth, strong, legitimate, together, love, job.\n",
    "\n",
    "In my interpretation, people tend to believe that their ideal candidate is truthful, legitimate, above good and bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Negative words\")\n",
    "wordcloud_draw(train_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, negative tweets contains words like: influence, news, elevator music, disappointing, softball, makeup, cherry picking, trying\n",
    "\n",
    "In my understanding people missed the decisively acting and considered the scolded candidates too soft and cherry picking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyage des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
    "    words_cleaned = [word for word in words_filtered\n",
    "        if 'http' not in word\n",
    "        and not word.startswith('@')\n",
    "        and not word.startswith('#')\n",
    "        and word != 'RT']\n",
    "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n",
    "    tweets.append((words_without_stopwords, row.sentiment))\n",
    "\n",
    "test_pos = test[ test['sentiment'] == 'Positive']\n",
    "test_pos = test_pos['text']\n",
    "test_neg = test[ test['sentiment'] == 'Negative']\n",
    "test_neg = test_neg['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des features à partir du texte en se basant sur la fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize des mots\n",
    "def get_words_in_tweets(tweets):\n",
    "    all = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all.extend(words)\n",
    "    return all\n",
    "#Features\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n",
    "\n",
    "w_features = get_word_features(get_words_in_tweets(tweets))\n",
    "\n",
    "#Features ==> Dictionnaire\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les features les plus apparentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_draw(w_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features,tweets)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat du training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cnt = 0\n",
    "pos_cnt = 0\n",
    "for obj in test_neg: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Negative'): \n",
    "        neg_cnt = neg_cnt + 1\n",
    "for obj in test_pos: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Positive'): \n",
    "        pos_cnt = pos_cnt + 1\n",
    "        \n",
    "print('[Negative]: %s/%s '  % (len(test_neg),neg_cnt))        \n",
    "print('[Positive]: %s/%s '  % (len(test_pos),pos_cnt))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse sentimentale sur YouTube Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sur les commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize des mots\n",
    "def get_words_in_youtube(yt_comments):\n",
    "    all = []\n",
    "    for comment in yt_comments:\n",
    "        all.extend(comment.split())\n",
    "    return all\n",
    "\n",
    "#Features\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_features = get_word_features(get_words_in_youtube(list(df_comments.textDisplay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features ==> Dictionnaire\n",
    "def extract_features_yt(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in y_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_draw(y_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cnt = 0\n",
    "pos_cnt = 0\n",
    "pol = []\n",
    "for obj in df_comments.textDisplay:\n",
    "    res =  classifier.classify(extract_features_yt(obj.split()))\n",
    "    if(res == 'Negative'): \n",
    "        neg_cnt = neg_cnt + 1\n",
    "    if(res == 'Positive'): \n",
    "        pos_cnt = pos_cnt + 1\n",
    "    pol.append(res)\n",
    "        \n",
    "print('[Negative]: %s '  % neg_cnt)       \n",
    "print('[Positive]: %s '  % pos_cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_com_pol = pd.DataFrame(dict(polarity = pol,comments = list(df_comments.textDisplay.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_com_pol.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(yt_com_pol['polarity']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new ID for tweeets\n",
    "data.insert(2,'New_ID', range(0,len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nettoyage d'un text\n",
    "import re\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Utility function to clean the text in a comment or tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste des commentaires youtube nettoyés\n",
    "cleaned_yt = []\n",
    "for com in df_comments['textDisplay']:\n",
    "    cleaned_yt.append(clean_text(com))\n",
    "cleaned_yt[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting et suppression des stopwords\n",
    "yt_words = []\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "for phrase in cleaned_yt:\n",
    "    words_filtered = [e.lower() for e in phrase.split() if len(e) >= 3]\n",
    "    words_cleaned = [word for word in words_filtered\n",
    "        if 'http' not in word\n",
    "        and not word.startswith('<')\n",
    "        and not word.startswith('#')\n",
    "        and word != 'http']\n",
    "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n",
    "    yt_words.append(words_without_stopwords)\n",
    "yt_words[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de mots des tweets\n",
    "tweets_words = []\n",
    "for a,b in tweets:\n",
    "    tweets_words.append(a)\n",
    "print(tweets_words[:5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yt_words[0],tweets_words[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction d'une liste des aretes à base des triplets communs entre les commentaires et les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_aretes = []\n",
    "for i in range(len(yt_words)):\n",
    "    for j in range(len(tweets_words)):\n",
    "        if len(set(yt_words[i]).intersection(tweets_words[j])) > 2:\n",
    "            liste_aretes.append((i,j))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_aretes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(liste_aretes)\n",
    "plt.figure(figsize=(18,18))\n",
    "nx.draw(G, pos=nx.spring_layout(G,k=.12),node_color='k',edge_color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, \"graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
